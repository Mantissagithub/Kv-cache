{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPc8aMGhQ+MheiJe+LF+WWk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mantissagithub/Kv-cache/blob/main/Kv_cache_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOzz4k7v12nn",
        "outputId": "eb96af1e-50aa-4a43-dd3c-0ecec403eafa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2988, 0.0074, 0.0145]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Initialize the cache\n",
        "        self.cache = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Check if the input is in the cache\n",
        "        if str(x.tolist()) in self.cache:\n",
        "            return self.cache[str(x.tolist())]\n",
        "\n",
        "        out = self.fc1(x)\n",
        "        out = torch.sigmoid(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        # Add the output to the cache\n",
        "        self.cache[str(x.tolist())] = out\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Create an instance of the neural network\n",
        "model = SimpleNN(input_size=10, hidden_size=5, output_size=3)\n",
        "\n",
        "# Example input\n",
        "x = torch.randn(1, 10)\n",
        "\n",
        "# Forward pass\n",
        "output = model(x)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size must be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "        # Initialize the cache\n",
        "        self.cache = {'keys': [], 'values': []}\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, seq_length, _ = x.shape\n",
        "\n",
        "        # Compute keys and values\n",
        "        keys = self.keys(x)\n",
        "        values = self.values(x)\n",
        "\n",
        "        # Store keys and values in the cache\n",
        "        self.cache['keys'].append(keys)\n",
        "        self.cache['values'].append(values)\n",
        "\n",
        "        queries = self.queries(x)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nqk\", queries.view(N, seq_length, self.heads, self.head_dim),\n",
        "                               keys.view(N, seq_length, self.heads, self.head_dim))\n",
        "        attention = F.softmax(energy / (self.embed_size ** (1 / 2)), dim=2)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        out = torch.einsum(\"nqk,nkhd->nqhd\", attention, values.view(N, seq_length, self.heads, self.head_dim))\n",
        "        out = out.reshape(N, seq_length, self.embed_size)\n",
        "\n",
        "        return self.fc_out(out)\n",
        "\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, embed_size, heads, num_layers):\n",
        "        super(SimpleTransformer, self).__init__()\n",
        "        self.layers = nn.ModuleList(\n",
        "            [AttentionLayer(embed_size, heads) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    embed_size = 64  # Embedding size\n",
        "    heads = 8        # Number of attention heads\n",
        "    num_layers = 4   # Number of transformer layers\n",
        "\n",
        "    model = SimpleTransformer(embed_size, heads, num_layers)\n",
        "\n",
        "    # Example input (batch_size=1, seq_length=10, embed_size=64)\n",
        "    x = torch.randn(1, 10, embed_size)\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(x)\n",
        "    print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98guaPOx23_B",
        "outputId": "e4e2d2b5-93f5-4b68-cb5e-83929b924a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0409, -0.0239, -0.0342, -0.0294, -0.1071, -0.1014, -0.0431,\n",
            "          -0.0723,  0.0588,  0.0783, -0.0268,  0.0219,  0.0176,  0.0707,\n",
            "           0.0550, -0.0693,  0.1000, -0.0458, -0.0565,  0.0418, -0.0349,\n",
            "           0.0107, -0.0377,  0.0795, -0.0905, -0.1048, -0.0808, -0.0035,\n",
            "          -0.1311, -0.0102, -0.0910,  0.1705,  0.1249, -0.1070, -0.0981,\n",
            "          -0.0302, -0.0847, -0.0929, -0.0434,  0.0531,  0.0366, -0.0235,\n",
            "          -0.0118,  0.0608,  0.0411, -0.0035,  0.0136, -0.1386,  0.0021,\n",
            "           0.0931, -0.0306,  0.0929, -0.0049,  0.1363, -0.1158,  0.0302,\n",
            "          -0.0619,  0.1215, -0.0514,  0.1102,  0.0071,  0.0136, -0.1052,\n",
            "           0.1326],\n",
            "         [-0.0409, -0.0239, -0.0342, -0.0294, -0.1071, -0.1014, -0.0431,\n",
            "          -0.0723,  0.0588,  0.0783, -0.0268,  0.0219,  0.0176,  0.0707,\n",
            "           0.0550, -0.0693,  0.1000, -0.0458, -0.0565,  0.0418, -0.0349,\n",
            "           0.0107, -0.0377,  0.0795, -0.0905, -0.1048, -0.0808, -0.0035,\n",
            "          -0.1311, -0.0102, -0.0910,  0.1705,  0.1249, -0.1070, -0.0981,\n",
            "          -0.0302, -0.0847, -0.0929, -0.0434,  0.0531,  0.0366, -0.0235,\n",
            "          -0.0118,  0.0608,  0.0411, -0.0035,  0.0136, -0.1386,  0.0021,\n",
            "           0.0931, -0.0306,  0.0929, -0.0049,  0.1363, -0.1158,  0.0302,\n",
            "          -0.0619,  0.1215, -0.0514,  0.1102,  0.0071,  0.0136, -0.1052,\n",
            "           0.1326],\n",
            "         [-0.0409, -0.0239, -0.0342, -0.0294, -0.1071, -0.1014, -0.0431,\n",
            "          -0.0723,  0.0588,  0.0783, -0.0268,  0.0219,  0.0176,  0.0707,\n",
            "           0.0550, -0.0693,  0.1000, -0.0458, -0.0565,  0.0418, -0.0349,\n",
            "           0.0107, -0.0377,  0.0795, -0.0905, -0.1048, -0.0808, -0.0035,\n",
            "          -0.1311, -0.0102, -0.0910,  0.1705,  0.1249, -0.1070, -0.0981,\n",
            "          -0.0302, -0.0847, -0.0929, -0.0434,  0.0531,  0.0366, -0.0235,\n",
            "          -0.0118,  0.0608,  0.0411, -0.0035,  0.0136, -0.1386,  0.0021,\n",
            "           0.0931, -0.0306,  0.0929, -0.0049,  0.1363, -0.1158,  0.0302,\n",
            "          -0.0619,  0.1215, -0.0514,  0.1102,  0.0071,  0.0136, -0.1052,\n",
            "           0.1326],\n",
            "         [-0.0409, -0.0239, -0.0342, -0.0294, -0.1071, -0.1014, -0.0431,\n",
            "          -0.0723,  0.0588,  0.0783, -0.0268,  0.0219,  0.0176,  0.0707,\n",
            "           0.0550, -0.0693,  0.1000, -0.0458, -0.0565,  0.0418, -0.0349,\n",
            "           0.0107, -0.0377,  0.0795, -0.0905, -0.1048, -0.0808, -0.0035,\n",
            "          -0.1311, -0.0102, -0.0910,  0.1705,  0.1249, -0.1070, -0.0981,\n",
            "          -0.0302, -0.0847, -0.0929, -0.0434,  0.0531,  0.0366, -0.0235,\n",
            "          -0.0118,  0.0608,  0.0411, -0.0035,  0.0136, -0.1386,  0.0021,\n",
            "           0.0931, -0.0306,  0.0929, -0.0049,  0.1363, -0.1158,  0.0302,\n",
            "          -0.0619,  0.1215, -0.0514,  0.1102,  0.0071,  0.0136, -0.1052,\n",
            "           0.1326],\n",
            "         [-0.0409, -0.0239, -0.0342, -0.0294, -0.1071, -0.1014, -0.0431,\n",
            "          -0.0723,  0.0588,  0.0783, -0.0268,  0.0219,  0.0176,  0.0707,\n",
            "           0.0550, -0.0693,  0.1000, -0.0458, -0.0565,  0.0418, -0.0349,\n",
            "           0.0107, -0.0377,  0.0795, -0.0905, -0.1048, -0.0808, -0.0035,\n",
            "          -0.1311, -0.0102, -0.0910,  0.1705,  0.1249, -0.1070, -0.0981,\n",
            "          -0.0302, -0.0847, -0.0929, -0.0434,  0.0531,  0.0366, -0.0235,\n",
            "          -0.0118,  0.0608,  0.0411, -0.0035,  0.0136, -0.1386,  0.0021,\n",
            "           0.0931, -0.0306,  0.0929, -0.0049,  0.1363, -0.1158,  0.0302,\n",
            "          -0.0619,  0.1215, -0.0514,  0.1102,  0.0071,  0.0136, -0.1052,\n",
            "           0.1326],\n",
            "         [-0.0409, -0.0239, -0.0342, -0.0294, -0.1071, -0.1014, -0.0431,\n",
            "          -0.0723,  0.0588,  0.0783, -0.0268,  0.0219,  0.0176,  0.0707,\n",
            "           0.0550, -0.0693,  0.1000, -0.0458, -0.0565,  0.0418, -0.0349,\n",
            "           0.0107, -0.0377,  0.0795, -0.0905, -0.1048, -0.0808, -0.0035,\n",
            "          -0.1311, -0.0102, -0.0910,  0.1705,  0.1249, -0.1070, -0.0981,\n",
            "          -0.0302, -0.0847, -0.0929, -0.0434,  0.0531,  0.0366, -0.0235,\n",
            "          -0.0118,  0.0608,  0.0411, -0.0035,  0.0136, -0.1386,  0.0021,\n",
            "           0.0931, -0.0306,  0.0929, -0.0049,  0.1363, -0.1158,  0.0302,\n",
            "          -0.0619,  0.1215, -0.0514,  0.1102,  0.0071,  0.0136, -0.1052,\n",
            "           0.1326],\n",
            "         [-0.0409, -0.0239, -0.0342, -0.0294, -0.1071, -0.1014, -0.0431,\n",
            "          -0.0723,  0.0588,  0.0783, -0.0268,  0.0219,  0.0176,  0.0707,\n",
            "           0.0550, -0.0693,  0.1000, -0.0458, -0.0565,  0.0418, -0.0349,\n",
            "           0.0107, -0.0377,  0.0795, -0.0905, -0.1048, -0.0808, -0.0035,\n",
            "          -0.1311, -0.0102, -0.0910,  0.1705,  0.1249, -0.1070, -0.0981,\n",
            "          -0.0302, -0.0847, -0.0929, -0.0434,  0.0531,  0.0366, -0.0235,\n",
            "          -0.0118,  0.0608,  0.0411, -0.0035,  0.0136, -0.1386,  0.0021,\n",
            "           0.0931, -0.0306,  0.0929, -0.0049,  0.1363, -0.1158,  0.0302,\n",
            "          -0.0619,  0.1215, -0.0514,  0.1102,  0.0071,  0.0136, -0.1052,\n",
            "           0.1326],\n",
            "         [-0.0409, -0.0239, -0.0342, -0.0294, -0.1071, -0.1014, -0.0431,\n",
            "          -0.0723,  0.0588,  0.0783, -0.0268,  0.0219,  0.0176,  0.0707,\n",
            "           0.0550, -0.0693,  0.1000, -0.0458, -0.0565,  0.0418, -0.0349,\n",
            "           0.0107, -0.0377,  0.0795, -0.0905, -0.1048, -0.0808, -0.0035,\n",
            "          -0.1311, -0.0102, -0.0910,  0.1705,  0.1249, -0.1070, -0.0981,\n",
            "          -0.0302, -0.0847, -0.0929, -0.0434,  0.0531,  0.0366, -0.0235,\n",
            "          -0.0118,  0.0608,  0.0411, -0.0035,  0.0136, -0.1386,  0.0021,\n",
            "           0.0931, -0.0306,  0.0929, -0.0049,  0.1363, -0.1158,  0.0302,\n",
            "          -0.0619,  0.1215, -0.0514,  0.1102,  0.0071,  0.0136, -0.1052,\n",
            "           0.1326],\n",
            "         [-0.0409, -0.0239, -0.0342, -0.0294, -0.1071, -0.1014, -0.0431,\n",
            "          -0.0723,  0.0588,  0.0783, -0.0268,  0.0219,  0.0176,  0.0707,\n",
            "           0.0550, -0.0693,  0.1000, -0.0458, -0.0565,  0.0418, -0.0349,\n",
            "           0.0107, -0.0377,  0.0795, -0.0905, -0.1048, -0.0808, -0.0035,\n",
            "          -0.1311, -0.0102, -0.0910,  0.1705,  0.1249, -0.1070, -0.0981,\n",
            "          -0.0302, -0.0847, -0.0929, -0.0434,  0.0531,  0.0366, -0.0235,\n",
            "          -0.0118,  0.0608,  0.0411, -0.0035,  0.0136, -0.1386,  0.0021,\n",
            "           0.0931, -0.0306,  0.0929, -0.0049,  0.1363, -0.1158,  0.0302,\n",
            "          -0.0619,  0.1215, -0.0514,  0.1102,  0.0071,  0.0136, -0.1052,\n",
            "           0.1326],\n",
            "         [-0.0409, -0.0239, -0.0342, -0.0294, -0.1071, -0.1014, -0.0431,\n",
            "          -0.0723,  0.0588,  0.0783, -0.0268,  0.0219,  0.0176,  0.0707,\n",
            "           0.0550, -0.0693,  0.1000, -0.0458, -0.0565,  0.0418, -0.0349,\n",
            "           0.0107, -0.0377,  0.0795, -0.0905, -0.1048, -0.0808, -0.0035,\n",
            "          -0.1311, -0.0102, -0.0910,  0.1705,  0.1249, -0.1070, -0.0981,\n",
            "          -0.0302, -0.0847, -0.0929, -0.0434,  0.0531,  0.0366, -0.0235,\n",
            "          -0.0118,  0.0608,  0.0411, -0.0035,  0.0136, -0.1386,  0.0021,\n",
            "           0.0931, -0.0306,  0.0929, -0.0049,  0.1363, -0.1158,  0.0302,\n",
            "          -0.0619,  0.1215, -0.0514,  0.1102,  0.0071,  0.0136, -0.1052,\n",
            "           0.1326]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}